<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Theory Sequence Modeling For the previous posts, we make prediction assuming input and output pairs \((x^{(i)}, y^{(i)})\) isindependent identically distributed(i.i.d).It means the previous result don">
<meta property="og:type" content="article">
<meta property="og:title" content="DLSys: Sequence Modeling and Recurrent Networks">
<meta property="og:url" content="http://example.com/2024/08/24/DLSys-Sequence-Modeling-and-Recurrent-Networks/index.html">
<meta property="og:site_name" content="MLSys Day One">
<meta property="og:description" content="Theory Sequence Modeling For the previous posts, we make prediction assuming input and output pairs \((x^{(i)}, y^{(i)})\) isindependent identically distributed(i.i.d).It means the previous result don">
<meta property="og:locale">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173117956.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826092058694.png">
<meta property="article:published_time" content="2024-08-24T09:22:54.000Z">
<meta property="article:modified_time" content="2024-08-27T01:51:16.455Z">
<meta property="article:author" content="xjh42">
<meta property="article:tag" content="rnn">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173117956.png">

<link rel="canonical" href="http://example.com/2024/08/24/DLSys-Sequence-Modeling-and-Recurrent-Networks/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>DLSys: Sequence Modeling and Recurrent Networks | MLSys Day One</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">MLSys Day One</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/08/24/DLSys-Sequence-Modeling-and-Recurrent-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="xjh42">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="MLSys Day One">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          DLSys: Sequence Modeling and Recurrent Networks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-24 17:22:54" itemprop="dateCreated datePublished" datetime="2024-08-24T17:22:54+08:00">2024-08-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-08-27 09:51:16" itemprop="dateModified" datetime="2024-08-27T09:51:16+08:00">2024-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/dlsys/" itemprop="url" rel="index"><span itemprop="name">dlsys</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="theory">Theory</h2>
<h3 id="sequence-modeling">Sequence Modeling</h3>
<p>For the previous posts, we make prediction assuming input and output
pairs <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> is
<strong>independent identically distributed(i.i.d)</strong>.It means the
previous result donnot affect current result. In pratice, many cases
where <strong>the input/output pairs are given in a specific
sequence</strong>, and we need to use the information about this
sequence to help us make predictions.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173117956.png" alt="image-20240824173117956" style="zoom:67%;" /></p>
<ul>
<li><strong>Part of speech tagging</strong>: Given a sequence of words,
determine the part of speech of each word.<strong>A word’s part of
speech depends on the context in which it is being used</strong>, not
just on the word itself.</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173242176.png"
alt="image-20240824173242176" />
<figcaption aria-hidden="true">image-20240824173242176</figcaption>
</figure>
<ul>
<li><strong>speech to text</strong>: Given a audio signal (assume we
even know the word boundaries, and map each segment to a fix-sized
vector descriptor), determine the corresponding transcription. Again,
context of the words is extremely important. Because many words'
pronunciation are same. (see e.g., any bad speech recognition system
that attempts to “wreck a nice beach”)</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173452738.png"
alt="image-20240824173452738" />
<figcaption aria-hidden="true">image-20240824173452738</figcaption>
</figure>
<ul>
<li><strong>autoregressive prediction</strong>: A special case of
sequential prediction where the elements to predict is the next element
in the sequence.Common e.g., in time series forecasting, language
modeling, and other use cases. We strongly rely on the context of the
sentance to predict the next word.</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173627222.png"
alt="image-20240824173627222" />
<figcaption aria-hidden="true">image-20240824173627222</figcaption>
</figure>
<h3 id="recurrent-neural-networks">Recurrent Neural Networks</h3>
<p>Recurrent neural networks (RNNs) is a model to save the sequence
model problem. RNN maintain a <strong>hidden state</strong> over time,
which is a function of the current input and previous hidden state. The
previous hidden state contains the context of the previous inputs.
Therefore, hidden state use the current input and a list of previous
inputs to make a prediction.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826092058694.png" alt="image-20240826092058694" style="zoom:50%;" />
<span class="math display">\[
h_t = f(W_{hh}h_{t-1} + W_{hx}x_t + b_h)
\]</span></p>
<p><span class="math display">\[
y_t = g(W_{yh} + b_y)
\]</span></p>
<p>Where <span class="math inline">\(f\)</span> and <span
class="math inline">\(g\)</span> are activation function. <span
class="math inline">\(W_{hh}\)</span> , <span
class="math inline">\(W_{hx}\)</span>, <span
class="math inline">\(W_{yh}\)</span> are weights, and <span
class="math inline">\(b_y\)</span>, <span
class="math inline">\(b_h\)</span> are bias term. And <span
class="math inline">\(x \in R^n\)</span>, <span class="math inline">\(y
\in R^{k}\)</span>, <span class="math inline">\(h_t \in R^d\)</span>,
<span class="math inline">\(W_{hh} \in R^{d \times d}\)</span>, <span
class="math inline">\(W_{yh} \in R^{k \times d}\)</span>, <span
class="math inline">\(W_{hx} \in R^{d \times n}\)</span>, <span
class="math inline">\(b_h \in R^d\)</span>, <span
class="math inline">\(b_y \in R^k\)</span>.</p>
<p>After we define the RNN model, the next question is how to train RNN?
Given a sequence of inputs and target outputs<span
class="math inline">\((x_1, ..., x_T, y^{*}_1, ..., y^{*}_T)\)</span>,
we can train an RNN using backpropagation through time, which just
involves “unrolling” the RNN over the length of the sequence, then
relying mostly on <strong>autodiff</strong>. Without autodiff, we cannot
solve the problem, because we cannot write the gradient of the rnn
model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">opt = Optimizer(params = (W_hh, W_hx, W_yh, b_h, b_y))</span><br><span class="line">h[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">l = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t = <span class="number">1</span>,...,T:</span><br><span class="line">  h[t] = f(W_hh * h[t-<span class="number">1</span>] + W_hx * x[t] + b_h)</span><br><span class="line">  y[t] = g(W_yh * h[t] + b_y)</span><br><span class="line">  l += Loss(y[t], y_star[t])</span><br><span class="line">l.backward()</span><br><span class="line">opt.step()</span><br></pre></td></tr></table></figure>
<p>As you can see, the challenge for training RNNs is similar to that of
training deep MLP networks, becasuse the sequence maybe long and the rnn
is complicated.</p>
<ul>
<li><p><strong>Exploding activations/gradients</strong>: Because we
train RNNs on long sequences, if the weights/activation of the RNN are
scaled poorly, the hidden activations (and therefore also the gradients)
will grow unboundedly with sequence length. For example, we use below
initialization, the gradient will soon be NaN which cannot be stored in
the 32-bit floating number.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826094116833.png"
alt="image-20240826094116833" />
<figcaption aria-hidden="true">image-20240826094116833</figcaption>
</figure></li>
<li><p><strong>Vanishing activation/gradients</strong>: Similarly, if
weights are too small then information from the inputs will quickly
decay with time (and it is precisely the “long range” dependencies that
we would often like to model with sequence models). So the context of
the previous inputs will decay.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826094344394.png"
alt="image-20240826094344394" />
<figcaption aria-hidden="true">image-20240826094344394</figcaption>
</figure></li>
</ul>
<p>To solve <strong>Exploding activations/gradients</strong> problem, we
can use other activation functions. ReLU is a bad activation function
because it can grow unboundedly. We can use sigmod and tanh activation
function.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826094802233.png"
alt="image-20240826094802233" />
<figcaption aria-hidden="true">image-20240826094802233</figcaption>
</figure>
<p>But the problem <strong>Vanishing activation/gradients</strong> still
be unsolved. Creating large enough weights to not cause
activations/gradients to vanish requires being in <strong>the
“saturating” regions of the activations</strong>, where gradients are
very small ⟹ still have vanishing gradients</p>
<p>How solve this problems? Use LSTM!</p>
<h3 id="lstms">LSTMs</h3>
<p>Long short term memory (LSTM) cells are a particular form of hidden
unit update that avoids (some of) the problems of vanilla LSTMs. It make
two changes to avoid vanishing activation/gradients.</p>
<ul>
<li><p>Step 1: Divide the hidden unit into two components, called
(confusingly) the <strong>hidden state</strong> and the <strong>cell
state</strong></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826095305637.png"
alt="image-20240826095305637" />
<figcaption aria-hidden="true">image-20240826095305637</figcaption>
</figure>
<ul>
<li><p>Step 2: Use a very specific formula to update the hidden state
and cell state (throwing in some other names, like “forget gate”, “input
gate”, “output gate” for good measure)</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826095513339.png"
alt="image-20240826095513339" />
<figcaption aria-hidden="true">image-20240826095513339</figcaption>
</figure>
<p>where <span class="math inline">\(i_t \in R^d\)</span>, <span
class="math inline">\(f_t \in R^d\)</span>,<span
class="math inline">\(g_t \in R^d\)</span>, <span
class="math inline">\(o_t \in R^d\)</span>, <span
class="math inline">\(W_{hh} \in R^{4d \times d}\)</span>, <span
class="math inline">\(h_t \in R^d\)</span> , <span
class="math inline">\(W_{hx} \in R^{4d \times n}\)</span></p></li>
</ul></li>
</ul>
<p>Why LSTM works? The factor of <span
class="math inline">\(f_t\)</span> and <span
class="math inline">\(i_t\)</span> can control the context information.
Close to 0 --&gt; not mantain the context, Close 1 --&gt; context
information will be untoched.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826100216569.png"
alt="image-20240826100216569" />
<figcaption aria-hidden="true">image-20240826100216569</figcaption>
</figure>
<h3 id="beyond-simple-sequential-models">Beyond "simple" sequential
Models</h3>
<p>We'll introduce a list of aplication of RNN.</p>
<ul>
<li><p><strong>Seq2Seq model</strong>: To give you a short glimpse of
the kind of things you can do with RNNs/LSTMs beyond “simple” sequence
prediction, consider the task of <strong>trying to translate between
languages</strong>.</p>
<p>Can concatenate two RNNs together, one that “only” processes the
sequence to create a final hidden state (i.e., no loss function,
encoder); then a section that takes in this initial hidden state, and
“only” generates a sequence(decoder). Why this model works? Because the
translation task is not a one-one mapping problem.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826100555240.png"
alt="image-20240826100555240" />
<figcaption aria-hidden="true">image-20240826100555240</figcaption>
</figure>
<p><span class="math inline">\(h_5\)</span> contains the summary of the
context.</p></li>
<li><p><strong>Bidirectional RNNs</strong>: RNNs can use only the
sequence information up until time <span
class="math inline">\(t\)</span> to predict <span
class="math inline">\(y_t\)</span>.This is sometimes desirable (e.g.,
autoregressive models). But sometime undesirable (e.g., language
translation where we want to use “whole” input sequence)</p>
<p>Bi-directional RNNs stack a forwardrunning RNN with a
backward-running RNN: information from the entire sequence to propagates
to the hidden state. So we can use the full context to predict!</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826101002495.png"
alt="image-20240826101002495" />
<figcaption aria-hidden="true">image-20240826101002495</figcaption>
</figure></li>
</ul>
<h2 id="implementing-rnns">Implementing RNNs</h2>
<p>Codelab notebook links: <a
target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1c8fmSa1H9noi_1RJhFOksloEFNDrEmU7?usp=sharing">implementing
RNNs</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/rnn/" rel="tag"># rnn</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2024/08/27/DLSys-Transformers-and-Autoregressive-Models/" rel="next" title="DLSys: Transformers and Attention">
      DLSys: Transformers and Attention <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#theory"><span class="nav-number">1.</span> <span class="nav-text">Theory</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sequence-modeling"><span class="nav-number">1.1.</span> <span class="nav-text">Sequence Modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#recurrent-neural-networks"><span class="nav-number">1.2.</span> <span class="nav-text">Recurrent Neural Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lstms"><span class="nav-number">1.3.</span> <span class="nav-text">LSTMs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#beyond-simple-sequential-models"><span class="nav-number">1.4.</span> <span class="nav-text">Beyond &quot;simple&quot; sequential
Models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#implementing-rnns"><span class="nav-number">2.</span> <span class="nav-text">Implementing RNNs</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">xjh42</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xjh42</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
