<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>DLSys: Transformers and Attention</title>
    <url>/2024/08/27/DLSys-Transformers-and-Autoregressive-Models/</url>
    <content><![CDATA[<h2 id="theory">Theory</h2>
<h3 id="the-two-approaches-to-time-series-modeling">The two approaches
to time series modeling</h3>
<p>Let’s recall our basic time series prediction task from the previous
posts. More fundamentally, a time series prediction task is the task of
predicting: <span class="math display">\[
y_{1:T}=f_\theta(x_{1:T})
\]</span>
<img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240828130234284.png" alt="image-20240828130234284" style="zoom:50%;" /></p>
<p>where <span class="math inline">\(y_t\)</span> can depend only on
<span class="math inline">\(x_{1:t}\)</span>. There are mainly two
approach to do so: <strong>latent state approach</strong> and
<strong>direct prediction</strong>.</p>
<h4 id="the-rnn-latent-state-approach">The RNN “latent state”
approach</h4>
<p>We have already seen the RNN approach to time series:
<strong>maintain “latent state” <span class="math inline">\(h_t\)</span>
that summarizes all information up until that point</strong>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240828130718884-20240828130724791.png" alt="image-20240828130718884" style="zoom: 33%;" /></p>
<ul>
<li><strong>Pros</strong>: Potentially “infinite” history, compact
representation. It means the time series length is not limited and has
low memory to store the previous context.</li>
<li><strong>Cons</strong>: Long “compute path” between history and
current time ⟹ vanishing / exploding gradients, hard to learn. A single
state <span class="math inline">\(h_t\)</span> is hard to represent long
context, easily lost the previous context.</li>
</ul>
<h4 id="the-direct-prediction-approach">The “direct prediction”
approach</h4>
<p>To avoid vanishing/exploding gradients(lose context/context is hard
to store), we can also directly predict output <span
class="math inline">\(y_t\)</span>: <span class="math display">\[
y_t = f_\theta(x_{1:t})
\]</span> <span class="math inline">\(f_\theta\)</span> must be a
function that can make predictions of differently-sized inputs.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240828131152642.png" alt="image-20240828131152642" style="zoom:33%;" /></p>
<ul>
<li><strong>Pros</strong>: Often can map from past to current state with
shorter compute path. With a proper function <span
class="math inline">\(f_\theta\)</span> , we can avoid
vanishing/exploding gradients.</li>
<li><strong>Cons</strong>: No compact state representation, finite
history in practice.</li>
</ul>
<p>One of the most straightforward ways to specify the function <span
class="math inline">\(f_\theta\)</span>: (fully) convolutional networks,
a.k.a. temporal convolutional networks (TCNs). The main constraint is
that <strong>the convolutions be causal</strong>: <span
class="math inline">\(z^{i+1}_t\)</span> can only depend on <span
class="math inline">\(z^{i}_{t-k:t}\)</span>.</p>
<p>Many successful applications: e.g. WaveNet for speech generation (<a
href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/">van
den Oord et al., 2016</a>)</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240828132255771.png" alt="image-20240828132255771" style="zoom:67%;" /></p>
<p>Despite their simplicity, CNNs have a notable disadvantage for time
series prediction: the receptive field of each convolution is usually
relatively small ⟹ need deep networks to actually incorporate past
information. For example, WaveNet can achive a receptive field of 16
using 4 layer. For very long sequence, it will need very deep network.
There are several solutions:</p>
<ul>
<li><strong>Increase kernel size</strong>: also increases the parameters
of the network</li>
<li><a
href="https://medium.com/@abhishekjainindore24/pooling-and-their-types-in-cnn-4a4b8a7a4611"><strong>Pooling
layers</strong></a>: not as well suited to dense prediction, where we
want to predict all of <span class="math inline">\(y_{1:T}\)</span>.
We'll lose some predictions because pooling will decrease the size of
input.(alose decrease the size of output)</li>
<li><strong>Dilated convolutions</strong>: “Skips over” some past state
/ inputs. But we'll lose some context which may be important.</li>
</ul>
<p>As we can see, CNN is not well suited for time series prediction.
We'll introduce a new arch of network: transformer which will overcome
the cons of CNN.</p>
<h3 id="self-attention-and-transformers">Self-attention and
Transformers</h3>
<h4 id="self-attention">Self Attention</h4>
<p>Let's first talk about the important part of transformer:
<strong>Self Attention</strong>! “Attention” in deep networks generally
refers to <strong>any mechanism where individual states are weighted and
then combined</strong>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829085656637.png" alt="image-20240829085656637" style="zoom:50%;" /></p>
<p>Attention is used originally in RNNs when one wanted to combine
latent states over all times in a more general manner than “just”
looking at the last state. Let's define general attention in math: <span
class="math display">\[
z_t = \theta^Th_t^k
\]</span></p>
<p><span class="math display">\[
w = softmax(z)
\]</span></p>
<p><span class="math display">\[
\bar{h} = \sum_{t=1}^T(w_th_t^k)
\]</span></p>
<blockquote>
<p>The <strong>softmax function</strong> converts a vector of <em>K</em>
real numbers into a <a
href="https://en.wikipedia.org/wiki/Probability_distribution">probability
distribution</a> of <em>K</em> possible outcomes.</p>
</blockquote>
<p><strong>Self-attention</strong> refers to a particular form of
attention mechanism. Given three inputs <span
class="math inline">\(K,Q,V \in R^{T \times d}\)</span>, (“queries”,
“keys”, “values”, in one of the least-meaningful semantic designations
we have in deep learning).</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829090838397.png" alt="image-20240829090838397" /></p>
<p>we define the self attention operation as: <span
class="math display">\[
SelfAttetion(Q,K,V)=softmax(\frac{QK^T}{d^{1/2}})V
\]</span> Where the input is <span class="math inline">\(X \in R^{T
\times n}, W_k \in W^{n \times d}, W_Q \in W^{n \times d}, W_V \in W^{n
\times d}\)</span>, we can simple calculate <span
class="math inline">\(Q, K, V\)</span> as follows: <span
class="math display">\[
Q = XW_Q
\]</span></p>
<p><span class="math display">\[
K = XW_K
\]</span></p>
<p><span class="math display">\[
V = XW_V
\]</span></p>
<p>Compare to the attention used in rnn, we use input to replace the
hidden state <span class="math inline">\(h\)</span>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829092012276.png" alt="image-20240829092012276" style="zoom:50%;" /></p>
<p>Properties of self-attention:</p>
<ul>
<li>Invariant (really, equivariant) to permutations of the <span
class="math inline">\(Q, K, V\)</span>matrices</li>
<li>Allows influence between <span class="math inline">\(q_t,k_t,
v_t\)</span>over all times without increase parameter size.(compare to
CNN, in order to increase reception field, we need to increase kernel
size --&gt; increase parameter size)</li>
<li>Compute cost is <span class="math inline">\(O(T^2 + 2Td)\)</span>
(cannot be easily reduced due to nonlinearity applied to full <span
class="math inline">\(T \times T\)</span> matrix)</li>
<li>softmax const <span class="math inline">\(T^2\)</span></li>
<li>two matrix multiplication cost <span
class="math inline">\(Td\)</span></li>
</ul>
<h4 id="transformer">Transformer</h4>
<p>A simple transoformer block consist of self-attention mechanism and
other network blocks.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829092949134.png" alt="image-20240829092949134" style="zoom:50%;" /></p>
<p>In more detail, the Transformer block has the following form:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829093117580.png" alt="image-20240829093117580" style="zoom: 33%;" /></p>
<p>The Transformer architecture uses a series of attention mechanisms
(and feedfoward layers) to process a time series:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829093229868.png" alt="image-20240829093229868" style="zoom: 50%;" /></p>
<p>Which can be form use math equation below: <span
class="math display">\[
Z^{(i+1)}=TransformerBlock(Z^{(i)})
\]</span> All time steps (in practice, within a given time slice) are
<strong>processed in parallel</strong>, avoids the need for sequential
processing as in RNNs.</p>
<p>We can apply the Transformer block to the “direct” prediction method
for time series, instead of using a convolutional block.</p>
<ul>
<li>Pros:
<ul>
<li>Full receptive field within a single layer (i.e., can immediately
use past data)</li>
<li>Mixing over time doesn’t increase parameter count (unlike
convolutions)</li>
</ul></li>
<li>Cons:
<ul>
<li>All outputs depend on all inputs (no good e.g., for autoregressive
tasks) -- the latent cortex is more important.</li>
<li>No ordering of data (remember that transformers are equivariant to
permutations of the sequence)--the position masters.</li>
</ul></li>
</ul>
<p>To solve the cons of transformer, we introduce two techniques:
<strong>masked self-attention</strong>, and <strong>Positional
encodings</strong>.</p>
<h4 id="masked-self-attention">Masked self-attention</h4>
<p>To solve the problem of “acausal” dependencies, we can mask the
softmax operator to assign zero weight to any “future” time steps.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829094005050.png" alt="image-20240829094005050" style="zoom: 50%;" /></p>
<p>Note that even though technically this means we can “avoid” creating
those entries in the attention matrix to being with, in practice it’s
often faster to just form them then mask them out.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829094241280.png" alt="image-20240829094241280" style="zoom:50%;" /></p>
<h4 id="positional-encodings">Positional encodings</h4>
<p>To solve the problem of “order invariance”, we can add a positional
encoding to the input, which <strong>associates each input with its
position in the sequence</strong>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829094534849.png" alt="image-20240829094534849" style="zoom:50%;" /></p>
<p>and where <span class="math inline">\(w_i, i = 1,...,n\)</span> is
typically chosen according to a logarithmic schedule. Really, add
positional encoding to d-dimensional projection of <span
class="math inline">\(T\)</span></p>
<h3 id="transformers-beyond-time-series">Transformers beyond time
series</h3>
<p>Recent work has observed that transformer blocks are extremely
powerful beyond just time series</p>
<ul>
<li>Vision Transformers: Apply transformer to image (represented by a
collection of patch embeddings), works better than CNNs for large data
sets</li>
<li>Graph Transformers: Capture graph structure in the attention
matrix</li>
</ul>
<p>In all cases, some challenges are:</p>
<ul>
<li>How to represent data such that <span
class="math inline">\(O(T^2)\)</span> operations are feasible</li>
<li>How to form positional embeddings</li>
<li>How to form the mask matrix</li>
</ul>
<h2 id="implementation">Implementation</h2>
<p>The runnable colab implementation is <a
href="https://colab.research.google.com/drive/1sSkoJhexTDEgdBahAIm2SeV0Eqc0frj2?usp=sharing">here</a></p>
]]></content>
      <categories>
        <category>dlsys</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>DLSys: Sequence Modeling and Recurrent Networks</title>
    <url>/2024/08/24/DLSys-Sequence-Modeling-and-Recurrent-Networks/</url>
    <content><![CDATA[<h2 id="theory">Theory</h2>
<h3 id="sequence-modeling">Sequence Modeling</h3>
<p>For the previous posts, we make prediction assuming input and output
pairs <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> is
<strong>independent identically distributed(i.i.d)</strong>.It means the
previous result donnot affect current result. In pratice, many cases
where <strong>the input/output pairs are given in a specific
sequence</strong>, and we need to use the information about this
sequence to help us make predictions.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173117956.png" alt="image-20240824173117956" style="zoom:67%;" /></p>
<ul>
<li><strong>Part of speech tagging</strong>: Given a sequence of words,
determine the part of speech of each word.<strong>A word’s part of
speech depends on the context in which it is being used</strong>, not
just on the word itself.</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173242176.png"
alt="image-20240824173242176" />
<figcaption aria-hidden="true">image-20240824173242176</figcaption>
</figure>
<ul>
<li><strong>speech to text</strong>: Given a audio signal (assume we
even know the word boundaries, and map each segment to a fix-sized
vector descriptor), determine the corresponding transcription. Again,
context of the words is extremely important. Because many words'
pronunciation are same. (see e.g., any bad speech recognition system
that attempts to “wreck a nice beach”)</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173452738.png"
alt="image-20240824173452738" />
<figcaption aria-hidden="true">image-20240824173452738</figcaption>
</figure>
<ul>
<li><strong>autoregressive prediction</strong>: A special case of
sequential prediction where the elements to predict is the next element
in the sequence.Common e.g., in time series forecasting, language
modeling, and other use cases. We strongly rely on the context of the
sentance to predict the next word.</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173627222.png"
alt="image-20240824173627222" />
<figcaption aria-hidden="true">image-20240824173627222</figcaption>
</figure>
<h3 id="recurrent-neural-networks">Recurrent Neural Networks</h3>
<p>Recurrent neural networks (RNNs) is a model to save the sequence
model problem. RNN maintain a <strong>hidden state</strong> over time,
which is a function of the current input and previous hidden state. The
previous hidden state contains the context of the previous inputs.
Therefore, hidden state use the current input and a list of previous
inputs to make a prediction.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826092058694.png" alt="image-20240826092058694" style="zoom:50%;" />
<span class="math display">\[
h_t = f(W_{hh}h_{t-1} + W_{hx}x_t + b_h)
\]</span></p>
<p><span class="math display">\[
y_t = g(W_{yh} + b_y)
\]</span></p>
<p>Where <span class="math inline">\(f\)</span> and <span
class="math inline">\(g\)</span> are activation function. <span
class="math inline">\(W_{hh}\)</span> , <span
class="math inline">\(W_{hx}\)</span>, <span
class="math inline">\(W_{yh}\)</span> are weights, and <span
class="math inline">\(b_y\)</span>, <span
class="math inline">\(b_h\)</span> are bias term. And <span
class="math inline">\(x \in R^n\)</span>, <span class="math inline">\(y
\in R^{k}\)</span>, <span class="math inline">\(h_t \in R^d\)</span>,
<span class="math inline">\(W_{hh} \in R^{d \times d}\)</span>, <span
class="math inline">\(W_{yh} \in R^{k \times d}\)</span>, <span
class="math inline">\(W_{hx} \in R^{d \times n}\)</span>, <span
class="math inline">\(b_h \in R^d\)</span>, <span
class="math inline">\(b_y \in R^k\)</span>.</p>
<p>After we define the RNN model, the next question is how to train RNN?
Given a sequence of inputs and target outputs<span
class="math inline">\((x_1, ..., x_T, y^{*}_1, ..., y^{*}_T)\)</span>,
we can train an RNN using backpropagation through time, which just
involves “unrolling” the RNN over the length of the sequence, then
relying mostly on <strong>autodiff</strong>. Without autodiff, we cannot
solve the problem, because we cannot write the gradient of the rnn
model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opt = Optimizer(params = (W_hh, W_hx, W_yh, b_h, b_y))</span><br><span class="line">h[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">l = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t = <span class="number">1</span>,...,T:</span><br><span class="line">  h[t] = f(W_hh * h[t-<span class="number">1</span>] + W_hx * x[t] + b_h)</span><br><span class="line">  y[t] = g(W_yh * h[t] + b_y)</span><br><span class="line">  l += Loss(y[t], y_star[t])</span><br><span class="line">l.backward()</span><br><span class="line">opt.step()</span><br></pre></td></tr></table></figure>
<p>As you can see, the challenge for training RNNs is similar to that of
training deep MLP networks, becasuse the sequence maybe long and the rnn
is complicated.</p>
<ul>
<li><p><strong>Exploding activations/gradients</strong>: Because we
train RNNs on long sequences, if the weights/activation of the RNN are
scaled poorly, the hidden activations (and therefore also the gradients)
will grow unboundedly with sequence length. For example, we use below
initialization, the gradient will soon be NaN which cannot be stored in
the 32-bit floating number.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826094116833.png"
alt="image-20240826094116833" />
<figcaption aria-hidden="true">image-20240826094116833</figcaption>
</figure></li>
<li><p><strong>Vanishing activation/gradients</strong>: Similarly, if
weights are too small then information from the inputs will quickly
decay with time (and it is precisely the “long range” dependencies that
we would often like to model with sequence models). So the context of
the previous inputs will decay.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826094344394.png"
alt="image-20240826094344394" />
<figcaption aria-hidden="true">image-20240826094344394</figcaption>
</figure></li>
</ul>
<p>To solve <strong>Exploding activations/gradients</strong> problem, we
can use other activation functions. ReLU is a bad activation function
because it can grow unboundedly. We can use sigmod and tanh activation
function.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826094802233.png"
alt="image-20240826094802233" />
<figcaption aria-hidden="true">image-20240826094802233</figcaption>
</figure>
<p>But the problem <strong>Vanishing activation/gradients</strong> still
be unsolved. Creating large enough weights to not cause
activations/gradients to vanish requires being in <strong>the
“saturating” regions of the activations</strong>, where gradients are
very small ⟹ still have vanishing gradients</p>
<p>How solve this problems? Use LSTM!</p>
<h3 id="lstms">LSTMs</h3>
<p>Long short term memory (LSTM) cells are a particular form of hidden
unit update that avoids (some of) the problems of vanilla LSTMs. It make
two changes to avoid vanishing activation/gradients.</p>
<ul>
<li><p>Step 1: Divide the hidden unit into two components, called
(confusingly) the <strong>hidden state</strong> and the <strong>cell
state</strong></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826095305637.png"
alt="image-20240826095305637" />
<figcaption aria-hidden="true">image-20240826095305637</figcaption>
</figure>
<ul>
<li><p>Step 2: Use a very specific formula to update the hidden state
and cell state (throwing in some other names, like “forget gate”, “input
gate”, “output gate” for good measure)</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826095513339.png"
alt="image-20240826095513339" />
<figcaption aria-hidden="true">image-20240826095513339</figcaption>
</figure>
<p>where <span class="math inline">\(i_t \in R^d\)</span>, <span
class="math inline">\(f_t \in R^d\)</span>,<span
class="math inline">\(g_t \in R^d\)</span>, <span
class="math inline">\(o_t \in R^d\)</span>, <span
class="math inline">\(W_{hh} \in R^{4d \times d}\)</span>, <span
class="math inline">\(h_t \in R^d\)</span> , <span
class="math inline">\(W_{hx} \in R^{4d \times n}\)</span></p></li>
</ul></li>
</ul>
<p>Why LSTM works? The factor of <span
class="math inline">\(f_t\)</span> and <span
class="math inline">\(i_t\)</span> can control the context information.
Close to 0 --&gt; not mantain the context, Close 1 --&gt; context
information will be untoched.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826100216569.png"
alt="image-20240826100216569" />
<figcaption aria-hidden="true">image-20240826100216569</figcaption>
</figure>
<h3 id="beyond-simple-sequential-models">Beyond "simple" sequential
Models</h3>
<p>We'll introduce a list of aplication of RNN.</p>
<ul>
<li><p><strong>Seq2Seq model</strong>: To give you a short glimpse of
the kind of things you can do with RNNs/LSTMs beyond “simple” sequence
prediction, consider the task of <strong>trying to translate between
languages</strong>.</p>
<p>Can concatenate two RNNs together, one that “only” processes the
sequence to create a final hidden state (i.e., no loss function,
encoder); then a section that takes in this initial hidden state, and
“only” generates a sequence(decoder). Why this model works? Because the
translation task is not a one-one mapping problem.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826100555240.png"
alt="image-20240826100555240" />
<figcaption aria-hidden="true">image-20240826100555240</figcaption>
</figure>
<p><span class="math inline">\(h_5\)</span> contains the summary of the
context.</p></li>
<li><p><strong>Bidirectional RNNs</strong>: RNNs can use only the
sequence information up until time <span
class="math inline">\(t\)</span> to predict <span
class="math inline">\(y_t\)</span>.This is sometimes desirable (e.g.,
autoregressive models). But sometime undesirable (e.g., language
translation where we want to use “whole” input sequence)</p>
<p>Bi-directional RNNs stack a forwardrunning RNN with a
backward-running RNN: information from the entire sequence to propagates
to the hidden state. So we can use the full context to predict!</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826101002495.png"
alt="image-20240826101002495" />
<figcaption aria-hidden="true">image-20240826101002495</figcaption>
</figure></li>
</ul>
<h2 id="implementing-rnns">Implementing RNNs</h2>
<p>Codelab notebook links: <a
href="https://colab.research.google.com/drive/1c8fmSa1H9noi_1RJhFOksloEFNDrEmU7?usp=sharing">implementing
RNNs</a></p>
]]></content>
      <categories>
        <category>dlsys</category>
      </categories>
      <tags>
        <tag>rnn</tag>
      </tags>
  </entry>
</search>
