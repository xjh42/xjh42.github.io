<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>DLSys: Convolutional Networks</title>
    <url>/2024/08/30/DLSys-Convolutional-Networks/</url>
    <content><![CDATA[<p>In this post, we'll introduce Convolutional Networks(CNN) with large
application.</p>
<h1 id="theory">Theory</h1>
<h2 id="convolution-operator">Convolution Operator</h2>
<p>So far we only consider fully connected networks, <strong>which treat
input images as vectors(size is <span class="math inline">\(n\)</span>),
and use a large weight matrix(<span class="math inline">\(W \in R^{n
\times d}\)</span>) to map input vector to a feature vector</strong>.
This creates a substantial problem as we attempt to handle larger
images: a 256x256 RGB image ‚üπ ~200K dimensional input ‚üπ mapping to 1000
dimensional hidden vector requires 200M parameters (for a single
layer)</p>
<p>Another problem is this operation <strong>does not capture any of the
‚Äúintuitive‚Äù invariances that we expect to have in images</strong> (e.g.,
shifting image one pixel leads to very different next layer). It means
we use full image pixels to predict single value in next layer.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830095205371.png" alt="image-20240830095205371" style="zoom:67%;" /></p>
<p>We'll introduce a new operator convolution to simplify deep
networks.</p>
<h3 id="convolutions-can-simplify-deep-networks">Convolutions can
‚ÄúSimplify‚Äù Deep Networks</h3>
<p>Convolutions combine two ideas that are well-suited to processing
images</p>
<ul>
<li>Require that <strong>activations between layers occur only in a
‚Äúlocal‚Äù manner</strong>, and treat hidden layers themselves as spatial
images</li>
<li>Share weights across all spatial locations</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830095715641.png" alt="image-20240830095715641" style="zoom:50%;" /></p>
<p>Compare to full connected network, convolution can:</p>
<ul>
<li>Drastically reduces the parameter count.(256x256 grayscale image ‚üπ
256x256 single-channel hidden layer: 4 billion parameters in fully
connected network to 9 parameters in 3x3 convolution)</li>
<li>Captures (some) ‚Äúnatural‚Äù invariances (Shifting input image one
pixel to the right shifts creates a hidden shifts the hidden unit
‚Äúimage‚Äù)</li>
</ul>
<p>Let's see how convolution works in details</p>
<h3 id="convolutions-in-detail">Convolutions in detail</h3>
<p>Convolutions are a basic primitive in many computer vision and image
processing algorithms. Convolution operator is to <strong>‚Äúslide‚Äù the
weights <span class="math inline">\(k \times k\)</span> weight <span
class="math inline">\(w\)</span> (called a filter, with kernel size
<span class="math inline">\(k\)</span>) over the image to produce a new
image, written <span class="math inline">\(y = z *
w\)</span></strong>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830100303185.png" alt="image-20240830100303185" style="zoom: 33%;" /></p>
<p>let's see how to compute <span
class="math inline">\(y_{11}\)</span>:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830124621705.png" alt="image-20240830124621705" style="zoom: 33%;" /></p>
<p>The rest value of <span class="math inline">\(y\)</span> is
calculated similarly.</p>
<h3 id="convolutions-in-image-processing">Convolutions in Image
Processing</h3>
<p>Convolutions (typically with prespecified filters) are a common
operation in many computer vision applications: convolution networks
just move to learned filters.</p>
<p>For conditional image programming, we use predefined filter, like
Gasssian Filter, Image gradient Filter.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830124932921.png" alt="image-20240830124932921" style="zoom: 33%;" /></p>
<h3 id="convolutions-in-deep-networks">Convolutions in deep
networks</h3>
<p>Convolutions in deep networks are virtually always multi-channel
convolutions: <strong>map multi-channel (e.g., RGB) inputs to
multi-channel hidden units</strong>. Multi-channel convolutions contain
a convolutional filter for each input-output channel pair, single output
channel is sum of convolutions over all input channels. It shows
below.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830125227168.png" alt="image-20240830125227168" style="zoom: 50%;" /></p>
<p>Let's see how to define it in math:</p>
<ul>
<li><span class="math inline">\(x \in R^{h \times w \times
c_{in}}\)</span> denotes <span class="math inline">\(c_{in}\)</span>
channel, size <span class="math inline">\(h \times w\)</span> image
input</li>
<li><span class="math inline">\(z \in R^{h \times w \times
c_{out}}\)</span> denotes <span class="math inline">\(c_{out}\)</span>
channel, size <span class="math inline">\(h \times w\)</span> image
out</li>
<li><span class="math inline">\(W \in R^{c_{in} \times c_{out} \times k
\times k}\)</span> (order 4 tensor) denotes convolutional filter</li>
</ul>
<p><span class="math display">\[
z[:,:,s] = \sum_{r=1}^{c_{in}}{x[:,:,r] * W[r,s,:,:]}
\]</span></p>
<p>The math equation is hard to understand. There is, in my view, a more
intuitive way to think about multi-channel convolutions: <strong>they
are a generalization of traditional convolutions with scalar
multiplications replaced by matrix-vector products.</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830130039529.png" alt="image-20240830130039529" style="zoom:80%;" /></p>
<h2 id="elements-of-practical-convolutions">Elements of Practical
Convolutions</h2>
<p>Naive convolution is hard to fit different condition. So there are
serveral techniques to make convolution more practical.</p>
<h3 id="padding">Padding</h3>
<p>‚ÄúNa√Øve‚Äù convolutions produce a smaller output than input image. If we
want to get a same resolution image, we need use padding technique.Be
careful, padding is only work for <strong>odd kernel size</strong>.</p>
<p>For (odd) kernel size <span class="math inline">\(k\)</span>, pad
input with <span class="math inline">\((k-1)/2\)</span> zeros on all
sides, results in an output that is the same size as the input</p>
<ul>
<li><p>There are serval variants like <strong>circular padding</strong>,
<strong>padding with mean values</strong>, etc</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830130634789.png" alt="image-20240830130634789" style="zoom:80%;" /></p></li>
</ul>
<h3 id="strided-convolutions-pooling">Strided Convolutions /
Pooling</h3>
<p>Given input matrix and filter, convolution output a fixed-size of
output matrix, don‚Äôt naively allow for representations at different
‚Äúresolutions‚Äù. If you want to a self-defined size of output matrix, you
can use either stride convolution or pooling techniques.</p>
<ul>
<li><p><strong>Pooling</strong> : incorporate max or average pooling
layers to aggregate information</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830131034673.png" alt="image-20240830131034673" style="zoom:50%;" /></p></li>
<li><p><strong>Strided Convolutions:</strong> slide convolutional filter
over image in increments &gt;1 (= stride)</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830131143353.png" alt="image-20240830131143353" style="zoom:50%;" /></p>
<h3 id="grouped-convolutions">Grouped Convolutions</h3>
<p>For large numbers of input/output channels, filters can still have
<strong>a large number of weights</strong>, can lead to
<strong>overfitting + slow computation</strong>.</p>
<p>To solve this, we can group together channels, so that <strong>groups
of channels in output only depend on corresponding groups of channels in
input </strong>(equivalently, enforce filter weight matrices to be
block-diagonal)</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830131419161.png" alt="image-20240830131419161" style="zoom:50%;" /></p>
<p>Given a simple example below, we can reduce filter parameter size
from <span class="math inline">\(R^{3 \times 3 \times k \times
k}\)</span> to <span class="math inline">\(R^{ k \times k}\)</span>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830131952146.png" alt="image-20240830131952146" style="zoom: 50%;" /></p>
<h3 id="dilations">Dilations</h3>
<p>Convolutions each have a <strong>relatively small receptive field
size</strong>. We'll lose the global context of the image.We can use
dilation to solve this problem.</p>
<p>Dilate (spread out) convolution filter, so that it covers more of the
image; note that getting an image of the same size again requires adding
more padding.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240830132217024.png" alt="image-20240830132217024" style="zoom:80%;" /></p></li>
</ul>
<h2 id="differentiating-convolutions">Differentiating Convolutions</h2>
<p>Recall the convolution math definition: <span class="math display">\[
z[:,:,s] = \sum_{r=1}^{c_{in}}{x[:,:,r] * W[r,s,:,:]}
\]</span></p>
<p>If we use the basic operations like multiply/add/sum to form
convolution and it's gradient. The computation graph will be large and
cost a lot of memory. So we want to define convolution as basic
operator. So it'll be only single node in a computation graph.</p>
<p>Recall that in order to integrate any operation into a deep network,
we need to be able to multiply by its partial derivatives (adjoint
operation). So if we define our operation: <span class="math display">\[
z = conv(x, W)
\]</span></p>
<p>how do we multiply by the adjoints: <span class="math display">\[
\bar{v}\frac{\partial conv(x, W)}{\partial x},\bar{v}\frac{\partial
conv(x, W)}{\partial W}
\]</span> Let‚Äôs consider the simpler case of a matrix-vector product
operation: <span class="math display">\[
z =Wx
\]</span></p>
<p>Then <span class="math inline">\(\frac{\partial z}{\partial x} =
W\)</span>, , so we need to compute the adjoint product: <span
class="math display">\[
\bar{v}^TW \iff W^T\bar{v}
\]</span></p>
<p>In other words, for a matrix vector multiply operation <span
class="math inline">\(Wx\)</span>, computing the backwards pass requires
multiplying by the transpose <span
class="math inline">\(W^T\)</span>.</p>
<p>In the next section, we can convert convolution to matmul operation,
then we can simply get the result of differentiation of convolution.</p>
<h3 id="convolutions-as-matrix-multiplication-version-1">Convolutions as
matrix multiplication: Version 1</h3>
<p>consider a 1D convolution to keep things a bit simpler:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240831123813936.png" alt="image-20240831123813936" style="zoom: 33%;" /></p>
<p>We can write a 1D convolution <span class="math inline">\(x *
w\)</span> (e.g., with zero padding) as a matrix multiplication <span
class="math inline">\(\hat{W}x\)</span> for some <span
class="math inline">\(\hat{W}\)</span> properly defined in terms of the
filter <span class="math inline">\(w\)</span>. <span
class="math display">\[
\begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ z_4 \\ z_5 \end{bmatrix} =
\begin{bmatrix} w_2 &amp; w_3 &amp; 0  &amp; 0 &amp; 0 \\ w_1 &amp; w_2
&amp; w_3  &amp; 0 &amp; 0 \\ 0 &amp; w_1 &amp; w_2  &amp; w_3 &amp; 0
\\ 0 &amp; 0 &amp; w_1  &amp; w_2 &amp; w_3 \\ 0 &amp; 0 &amp; 0  &amp;
w_1 &amp; w_2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\
x_5 \end{bmatrix}
\]</span> By converting convolution to matmul, we can easily compute the
gradient of convolution. Just compute the transponse of <span
class="math inline">\(\hat{W}\)</span>:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240831215657389.png" alt="image-20240831215657389" style="zoom: 40%;" /></p>
<p>Notice that the operation <span
class="math inline">\(\hat{W}^Tv\)</span> it itself just a convolution
with the ‚Äúflipped‚Äù filter: <span class="math inline">\([w_3 \space w_2
\space w_1]\)</span> ==&gt; <strong>adjoint operator <span
class="math inline">\(\bar{v}\frac{\partial conv(x, W)}{\partial
x}\)</span> just requires convolving <span
class="math inline">\(\bar{v}\)</span> with a the flipped <span
class="math inline">\(W\)</span></strong>.</p>
<h3 id="convolutions-as-matrix-multiplication-version-2">Convolutions as
matrix multiplication: Version 2</h3>
<p>What about the other adjoint,<span
class="math inline">\(\bar{v}\frac{\partial conv(x, W)}{\partial
W}\)</span>?</p>
<p>For this term, observe that we can also write the convolution as
<strong>a matrix-vector product treating the filter as the
vector</strong>. <span class="math display">\[
\begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ z_4 \\ z_5 \end{bmatrix} =
\begin{bmatrix} 0 &amp; x_1 &amp; x_2 \\ x_1 &amp; x_2 &amp; x_3 \\ x_2
&amp; x_3 &amp; x_4 \\ x_3 &amp; x_4 &amp; x_5 \\ x_4 &amp; x_5 &amp; 0
\end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \\ w_3 \end{bmatrix}
\]</span></p>
<p>So adjoint requires multiplying by the transpose of this <span
class="math inline">\(x\)</span>-based matrix: <span
class="math display">\[
\begin{bmatrix} 0 &amp; x_1 &amp; x_2 &amp; x_3 &amp; x_4 \\ x_1 &amp;
x_2 &amp; x_3 &amp; x_4 &amp; x_5 \\ x_2 &amp; x_3 &amp; x_4 &amp; x_5
&amp; 0 \end{bmatrix}
\]</span></p>
<h1 id="implementation">Implementation</h1>
<p>The reference implementation is <a
href="https://colab.research.google.com/drive/1N6MtkNXq6QOpGYhXTokOjWvkF3Ff9i8Z?usp=sharing">here</a></p>
]]></content>
      <categories>
        <category>dlsys</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>DLSys: ML Refresher / Softmax Regression</title>
    <url>/2024/09/24/DLSys-ML-Refresher-Softmax-Regression/</url>
    <content><![CDATA[<p>In this post, I'll introduce the machine learning basics.</p>
<h1 id="machine-learning-basics">Machine Learning Basics</h1>
<p>Suppose you want to write a program that will classify handwritten
drawing of digits into their appropriate category: 0,1,‚Ä¶,9. You could,
think hard about the nature of digits, try to determine the logic of
what indicates what kind of digit, and write a program to codify this
logic (Despite being a reasonable coder, I don‚Äôt think I could do this
very well)</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924091119545.png" alt="image-20240924091119545" style="zoom: 45%;" /></p>
<p>The (supervised) <strong>ML approach</strong>: collect a training set
of images with known labels and feed these into a machine learning
algorithm, which will (if done well), automatically produce a ‚Äúprogram‚Äù
that solves this task.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924091220614.png" alt="image-20240924091220614" style="zoom:50%;" /></p>
<p>Every machine learning algorithm consists of three different
elements:</p>
<ol type="1">
<li><strong>The hypothesis class(the model)</strong>: the ‚Äúprogram
structure‚Äù, parameterized via a set of parameters, that describes how we
map inputs (e.g., images of digits) to outputs (e.g., class labels, or
probabilities of different class labels)</li>
<li><strong>The loss function:</strong> a function that specifies how
‚Äúwell‚Äù a given hypothesis (i.e., a choice of parameters) performs on the
task of interest</li>
<li><strong>An optimization method:</strong> a procedure for determining
a set of parameters that (approximately) minimize the sum of losses over
the training set</li>
</ol>
<h1 id="softmax-regression">softmax regression</h1>
<p>Softmax regression is a basic method to solve <em>k-class
classification</em> problem. Let's first define the problem:</p>
<h2 id="multi-class-classification-setting">Multi-class classification
setting</h2>
<p>Let‚Äôs consider a k-class classification setting, where we have:</p>
<ul>
<li><strong>Training data</strong>: <span class="math inline">\(x^{(i)}
\in R^n\)</span>, and <span class="math inline">\(y^{(i)} \in
{1,...,k}\)</span> for <span class="math inline">\(i \in
{1,..,m}\)</span></li>
<li>ùëõ = dimensionality of the input data</li>
<li>ùëò = number of different classes / labels</li>
<li>ùëö = number of points in the training set</li>
</ul>
<p>Example: classification of 28x28 MNIST digits:</p>
<ul>
<li>ùëõ = 28 ‚ãÖ 28 = 784</li>
<li>ùëò = 10</li>
<li>ùëö = 60,000</li>
</ul>
<h2 id="three-element-of-softmax-regression">Three Element of Softmax
Regression</h2>
<h3 id="linear-hypothesis-function">Linear hypothesis function</h3>
<p>Our hypothesis function maps inputs <span class="math inline">\(x \in
R^n\)</span> to ùëò-dimensional vectors: <span class="math display">\[
h: R^n \to R^k
\]</span> where <span class="math inline">\(h_i(x), i \in
{1,..k}\)</span> indicates some measure of ‚Äúbelief‚Äù in how much likely
the label is to be class ùëñ (i.e., ‚Äúmost likely‚Äù prediction is coordinate
ùëñ with largest <span class="math inline">\(h_i(x)\)</span>.)</p>
<p>A <strong>linear hypothesis function</strong> uses a linear operator
(i.e. matrix multiplication) for this transformation: <span
class="math display">\[
h_\theta(x) = \theta^Tx
\]</span> for parameters <span class="math inline">\(\theta \in R^{n
\times k}\)</span></p>
<p>Often more convenient (and this is how you want to code things for
efficiency) to write the data and operations in <strong>matrix batch
form</strong>:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924093023427.png" alt="image-20240924093023427" style="zoom:33%;" /></p>
<p>Then the linear hypothesis applied to this batch can be written
as:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924093044172.png" alt="image-20240924093044172" style="zoom:33%;" /></p>
<h3 id="loss-function">Loss Function</h3>
<p>The simplest loss function to use in classification is just the
<strong>classification error</strong>, i.e., whether the classifier
makes a mistake a or not:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924093324238.png" alt="image-20240924093324238" style="zoom:33%;" /></p>
<p>We typically use this loss function to assess the quality of
classifiers.</p>
<p>Unfortunately, the error is a bad loss function to use for
optimization, i.e., selecting the best parameters, because it is not
differentiable.</p>
<h3 id="softmax-cross-entropy-loss">softmax / cross-entropy loss</h3>
<p>Let‚Äôs convert the hypothesis function to a ‚Äúprobability‚Äù by
<strong>exponentiating and normalizing its entries</strong> (to make
them all positive and sum to one): <span class="math display">\[
z_i = p(label=i) = \frac{exp(h_i(x))}{\sum_{j=1}^kexp(h_j(x))} =
norm(exp(h(y))
\]</span> Then let‚Äôs define a loss to be the (negative) log probability
of the true class: this is called softmax or cross-entropy loss : <span
class="math display">\[
l_{ce}(h(x),y) = -log(p(label=y)) = -h_y(x) + log\sum_{j=1}^kexp(h_j(x))
\]</span></p>
<h3
id="softmax-regression-optimization-method-stochastic-gradient-descent">Softmax
Regression Optimization Method: Stochastic Gradient Descent</h3>
<p>The third ingredient of a machine learning algorithm is a method for
solving the associated optimization problem, i.e., the problem of
minimizing the average loss on the training set:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924094501602.png" alt="image-20240924094501602" style="zoom:33%;" /></p>
<p>So how do we find Œò that solves this optimization problem?</p>
<p>For a matrix-input, scalar output function <span
class="math inline">\(f: R^{n \times k} \to R\)</span> the gradient is
defined as the matrix of partial derivatives:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924094719645.png" alt="image-20240924094719645" style="zoom: 67%;" /></p>
<p>Gradient points in the direction that most increases ùëì (locally).</p>
<p>To minimize a function, the gradient descent algorithm proceeds by
<strong>iteratively taking steps in the direction of the negative
gradient</strong>:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924094943710.png" alt="image-20240924094943710" style="zoom:50%;" /></p>
<p>where ùõº &gt; 0 is a step size or learning rate:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924095031145.png" alt="image-20240924095031145" style="zoom:50%;" /></p>
<p>If our objective (as is the case in machine learning) is the sum of
individual losses, we don‚Äôt want to compute the gradient using all
examples to make a single update to the parameters.</p>
<p>Instead, take many gradient steps each based upon a minibatch (small
partition of the data), to make many parameter updates using a single
‚Äúpass‚Äù over data</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924095218315.png" alt="image-20240924095218315" style="zoom:50%;" /></p>
<p>So, how do we compute the gradient for the softmax objective?</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924095421785.png" alt="image-20240924095421785" style="zoom:40%;" /></p>
<p>Let‚Äôs start by deriving the gradient of the softmax loss itself: for
vector <span class="math inline">\(h \in R^k\)</span>:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924095509447.png" alt="image-20240924095509447" style="zoom:33%;" /></p>
<p>So, in vector form: <span
class="math inline">\(\bigtriangledown_hl_{ce}(h,y)=z-e_y\)</span>,
where ùëß = norm(exp(h)).</p>
<p>Then, let‚Äôs compute the ‚Äúderivative‚Äù of the loss:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924100156250.png" alt="image-20240924100156250" style="zoom:33%;" /></p>
<p>So to make the dimensions work:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924100246907.png" alt="image-20240924100246907" style="zoom:33%;" /></p>
<p>Same process works if we use ‚Äúmatrix batch‚Äù form of the loss:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924100441453.png" alt="image-20240924100441453" style="zoom:33%;" /></p>
<h2 id="softmax-regression-all-in-one">Softmax Regression All in
One</h2>
<p>Despite a fairly complex derivation, we should highly just how simple
the final algorithm is:</p>
<ul>
<li>Repeat until parameters / loss converges
<ul>
<li>Iterative over minibatches <span class="math inline">\(X \in R^{B
\times n}, y \in \{1,...,k\}^B\)</span> of training set</li>
<li>Update the parameters <span class="math inline">\(\theta = \theta -
\frac{\alpha}{B}X^T(Z - I_y)\)</span></li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>dlsys</category>
      </categories>
      <tags>
        <tag>ml_basics</tag>
      </tags>
  </entry>
  <entry>
    <title>DLSys: Manual Neural Networks</title>
    <url>/2024/09/24/DLSys-Manual-Neural-Networks/</url>
    <content><![CDATA[<p>In this post, I'll introduce nerual network basics.</p>
<h1 id="nonlinear-hypothesis-classes">Nonlinear hypothesis classes</h1>
<p>Recall that we needed a hypothesis function to map inputs in <span
class="math inline">\(R^n\)</span> to outputs (class logits) in <span
class="math inline">\(R^k\)</span>, so we initially used the linear
hypothesis class. <span class="math display">\[
h_\theta(x) = \theta^Tx, \theta \in R^{n \times k}
\]</span> This classifier essentially forms k linear functions of the
input and then predicts the class with the largest value: equivalent to
partitioning the input into k linear regions corresponding to each
class.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924131137657.png" alt="image-20240924131137657" style="zoom:33%;" /></p>
<p>What if we have data that cannot be separated by a set of linear
regions? We want some way to separate these points via a nonlinear set
of class boundaries.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924131225307.png" alt="image-20240924131225307" style="zoom:33%;" /></p>
<p>We can apply a linear classifier to <strong>some (potentially
higher-dimensional) features of the data</strong>. <span
class="math display">\[
h_\theta(x) = \theta^T\phi(x), \theta \in R^{d \times k}, \phi: R^n \to
R^d
\]</span></p>
<h2 id="nonlinear-feature">Nonlinear Feature</h2>
<p>How can we create the feature function <span
class="math inline">\(\phi\)</span>?</p>
<ul>
<li>Through manual engineering of features relevant to the problem (the
‚Äúold‚Äù way of doing machine learning)</li>
<li>In a way that itself is learned from data (the ‚Äúnew‚Äù way of doing
ML)</li>
</ul>
<p>First take: what if we just again use a linear function for $<span
class="math inline">\(?\)</span>$ (x) = W^Tx $$</p>
<p>Doesn‚Äôt work, because it is just equivalent to another linear
classifier: <span class="math display">\[
h_\theta(x) = \theta^T\phi(x)=\theta^TW^Tx = \hat{\theta}x
\]</span> We can add nonlinear function the result like: <span
class="math display">\[
\phi(x) = \sigma(W^Tx)
\]</span> where <span class="math inline">\(W \in R^{n \times
d}\)</span>, and $ : R^d R^d $ is essentially any nonlinear
function.</p>
<p>Example: let W be a (fixed) matrix of random Gaussian samples, and
let $ $ be the cosine function ‚üπ ‚Äúrandom Fourier features‚Äù (work great
for many problems) But maybe we want to train $ W $ to minimize loss as
well? Or maybe we want to compose multiple features together?</p>
<h1 id="neural-networks">Neural networks</h1>
<p>A neural network refers to a particular type of hypothesis class,
consisting of <strong>multiple, parameterized differentiable functions
(a.k.a. ‚Äúlayers‚Äù) composed together in any manner to form the
output.</strong></p>
<h2 id="the-two-layer-neural-network">The ‚Äútwo layer‚Äù neural
network</h2>
<p>We can begin with the simplest form of neural network, basically just
the nonlinear features proposed earlier, but where both sets of weights
are learnable parameters.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240924132433576.png" alt="image-20240924132433576" style="zoom:33%;" />
<span class="math display">\[
h_\theta(x)=W_2^T\sigma(W_1^Tx), \theta = \{W_1 \in R^{n \times d},W_2
\in R^{d \times k } \}
\]</span></p>
<p>where <span class="math inline">\(\sigma: R \to R\)</span> is a
nonlinear function applied elementwise to the vector (e.g. sigmoid,
ReLU). Written in batch matrix form: <span class="math display">\[
h_\theta(X) = \sigma(XW_1)W_2
\]</span></p>
<h1 id="backpropagation">Backpropagation</h1>
]]></content>
      <categories>
        <category>dlsys</category>
      </categories>
      <tags>
        <tag>nn_basics</tag>
      </tags>
  </entry>
  <entry>
    <title>DLSys: Sequence Modeling and Recurrent Networks</title>
    <url>/2024/08/24/DLSys-Sequence-Modeling-and-Recurrent-Networks/</url>
    <content><![CDATA[<h2 id="theory">Theory</h2>
<h3 id="sequence-modeling">Sequence Modeling</h3>
<p>For the previous posts, we make prediction assuming input and output
pairs <span class="math inline">\((x^{(i)}, y^{(i)})\)</span> is
<strong>independent identically distributed(i.i.d)</strong>.It means the
previous result donnot affect current result. In pratice, many cases
where <strong>the input/output pairs are given in a specific
sequence</strong>, and we need to use the information about this
sequence to help us make predictions.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173117956.png" alt="image-20240824173117956" style="zoom:67%;" /></p>
<ul>
<li><strong>Part of speech tagging</strong>: Given a sequence of words,
determine the part of speech of each word.<strong>A word‚Äôs part of
speech depends on the context in which it is being used</strong>, not
just on the word itself.</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173242176.png"
alt="image-20240824173242176" />
<figcaption aria-hidden="true">image-20240824173242176</figcaption>
</figure>
<ul>
<li><strong>speech to text</strong>: Given a audio signal (assume we
even know the word boundaries, and map each segment to a fix-sized
vector descriptor), determine the corresponding transcription. Again,
context of the words is extremely important. Because many words'
pronunciation are same. (see e.g., any bad speech recognition system
that attempts to ‚Äúwreck a nice beach‚Äù)</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173452738.png"
alt="image-20240824173452738" />
<figcaption aria-hidden="true">image-20240824173452738</figcaption>
</figure>
<ul>
<li><strong>autoregressive prediction</strong>: A special case of
sequential prediction where the elements to predict is the next element
in the sequence.Common e.g., in time series forecasting, language
modeling, and other use cases. We strongly rely on the context of the
sentance to predict the next word.</li>
</ul>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240824173627222.png"
alt="image-20240824173627222" />
<figcaption aria-hidden="true">image-20240824173627222</figcaption>
</figure>
<h3 id="recurrent-neural-networks">Recurrent Neural Networks</h3>
<p>Recurrent neural networks (RNNs) is a model to save the sequence
model problem. RNN maintain a <strong>hidden state</strong> over time,
which is a function of the current input and previous hidden state. The
previous hidden state contains the context of the previous inputs.
Therefore, hidden state use the current input and a list of previous
inputs to make a prediction.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826092058694.png" alt="image-20240826092058694" style="zoom:50%;" />
<span class="math display">\[
h_t = f(W_{hh}h_{t-1} + W_{hx}x_t + b_h)
\]</span></p>
<p><span class="math display">\[
y_t = g(W_{yh} + b_y)
\]</span></p>
<p>Where <span class="math inline">\(f\)</span> and <span
class="math inline">\(g\)</span> are activation function. <span
class="math inline">\(W_{hh}\)</span> , <span
class="math inline">\(W_{hx}\)</span>, <span
class="math inline">\(W_{yh}\)</span> are weights, and <span
class="math inline">\(b_y\)</span>, <span
class="math inline">\(b_h\)</span> are bias term. And <span
class="math inline">\(x \in R^n\)</span>, <span class="math inline">\(y
\in R^{k}\)</span>, <span class="math inline">\(h_t \in R^d\)</span>,
<span class="math inline">\(W_{hh} \in R^{d \times d}\)</span>, <span
class="math inline">\(W_{yh} \in R^{k \times d}\)</span>, <span
class="math inline">\(W_{hx} \in R^{d \times n}\)</span>, <span
class="math inline">\(b_h \in R^d\)</span>, <span
class="math inline">\(b_y \in R^k\)</span>.</p>
<p>After we define the RNN model, the next question is how to train RNN?
Given a sequence of inputs and target outputs<span
class="math inline">\((x_1, ..., x_T, y^{*}_1, ..., y^{*}_T)\)</span>,
we can train an RNN using backpropagation through time, which just
involves ‚Äúunrolling‚Äù the RNN over the length of the sequence, then
relying mostly on <strong>autodiff</strong>. Without autodiff, we cannot
solve the problem, because we cannot write the gradient of the rnn
model.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">opt = Optimizer(params = (W_hh, W_hx, W_yh, b_h, b_y))</span><br><span class="line">h[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">l = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t = <span class="number">1</span>,...,T:</span><br><span class="line">  h[t] = f(W_hh * h[t-<span class="number">1</span>] + W_hx * x[t] + b_h)</span><br><span class="line">  y[t] = g(W_yh * h[t] + b_y)</span><br><span class="line">  l += Loss(y[t], y_star[t])</span><br><span class="line">l.backward()</span><br><span class="line">opt.step()</span><br></pre></td></tr></table></figure>
<p>As you can see, the challenge for training RNNs is similar to that of
training deep MLP networks, becasuse the sequence maybe long and the rnn
is complicated.</p>
<ul>
<li><p><strong>Exploding activations/gradients</strong>: Because we
train RNNs on long sequences, if the weights/activation of the RNN are
scaled poorly, the hidden activations (and therefore also the gradients)
will grow unboundedly with sequence length. For example, we use below
initialization, the gradient will soon be NaN which cannot be stored in
the 32-bit floating number.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826094116833.png"
alt="image-20240826094116833" />
<figcaption aria-hidden="true">image-20240826094116833</figcaption>
</figure></li>
<li><p><strong>Vanishing activation/gradients</strong>: Similarly, if
weights are too small then information from the inputs will quickly
decay with time (and it is precisely the ‚Äúlong range‚Äù dependencies that
we would often like to model with sequence models). So the context of
the previous inputs will decay.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826094344394.png"
alt="image-20240826094344394" />
<figcaption aria-hidden="true">image-20240826094344394</figcaption>
</figure></li>
</ul>
<p>To solve <strong>Exploding activations/gradients</strong> problem, we
can use other activation functions. ReLU is a bad activation function
because it can grow unboundedly. We can use sigmod and tanh activation
function.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826094802233.png"
alt="image-20240826094802233" />
<figcaption aria-hidden="true">image-20240826094802233</figcaption>
</figure>
<p>But the problem <strong>Vanishing activation/gradients</strong> still
be unsolved. Creating large enough weights to not cause
activations/gradients to vanish requires being in <strong>the
‚Äúsaturating‚Äù regions of the activations</strong>, where gradients are
very small ‚üπ still have vanishing gradients</p>
<p>How solve this problems? Use LSTM!</p>
<h3 id="lstms">LSTMs</h3>
<p>Long short term memory (LSTM) cells are a particular form of hidden
unit update that avoids (some of) the problems of vanilla LSTMs. It make
two changes to avoid vanishing activation/gradients.</p>
<ul>
<li><p>Step 1: Divide the hidden unit into two components, called
(confusingly) the <strong>hidden state</strong> and the <strong>cell
state</strong></p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826095305637.png"
alt="image-20240826095305637" />
<figcaption aria-hidden="true">image-20240826095305637</figcaption>
</figure>
<ul>
<li><p>Step 2: Use a very specific formula to update the hidden state
and cell state (throwing in some other names, like ‚Äúforget gate‚Äù, ‚Äúinput
gate‚Äù, ‚Äúoutput gate‚Äù for good measure)</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826095513339.png"
alt="image-20240826095513339" />
<figcaption aria-hidden="true">image-20240826095513339</figcaption>
</figure>
<p>where <span class="math inline">\(i_t \in R^d\)</span>, <span
class="math inline">\(f_t \in R^d\)</span>,<span
class="math inline">\(g_t \in R^d\)</span>, <span
class="math inline">\(o_t \in R^d\)</span>, <span
class="math inline">\(W_{hh} \in R^{4d \times d}\)</span>, <span
class="math inline">\(h_t \in R^d\)</span> , <span
class="math inline">\(W_{hx} \in R^{4d \times n}\)</span></p></li>
</ul></li>
</ul>
<p>Why LSTM works? The factor of <span
class="math inline">\(f_t\)</span> and <span
class="math inline">\(i_t\)</span> can control the context information.
Close to 0 --&gt; not mantain the context, Close 1 --&gt; context
information will be untoched.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826100216569.png"
alt="image-20240826100216569" />
<figcaption aria-hidden="true">image-20240826100216569</figcaption>
</figure>
<h3 id="beyond-simple-sequential-models">Beyond "simple" sequential
Models</h3>
<p>We'll introduce a list of aplication of RNN.</p>
<ul>
<li><p><strong>Seq2Seq model</strong>: To give you a short glimpse of
the kind of things you can do with RNNs/LSTMs beyond ‚Äúsimple‚Äù sequence
prediction, consider the task of <strong>trying to translate between
languages</strong>.</p>
<p>Can concatenate two RNNs together, one that ‚Äúonly‚Äù processes the
sequence to create a final hidden state (i.e., no loss function,
encoder); then a section that takes in this initial hidden state, and
‚Äúonly‚Äù generates a sequence(decoder). Why this model works? Because the
translation task is not a one-one mapping problem.</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826100555240.png"
alt="image-20240826100555240" />
<figcaption aria-hidden="true">image-20240826100555240</figcaption>
</figure>
<p><span class="math inline">\(h_5\)</span> contains the summary of the
context.</p></li>
<li><p><strong>Bidirectional RNNs</strong>: RNNs can use only the
sequence information up until time <span
class="math inline">\(t\)</span> to predict <span
class="math inline">\(y_t\)</span>.This is sometimes desirable (e.g.,
autoregressive models). But sometime undesirable (e.g., language
translation where we want to use ‚Äúwhole‚Äù input sequence)</p>
<p>Bi-directional RNNs stack a forwardrunning RNN with a
backward-running RNN: information from the entire sequence to propagates
to the hidden state. So we can use the full context to predict!</p>
<figure>
<img
src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240826101002495.png"
alt="image-20240826101002495" />
<figcaption aria-hidden="true">image-20240826101002495</figcaption>
</figure></li>
</ul>
<h2 id="implementing-rnns">Implementing RNNs</h2>
<p>Codelab notebook links: <a
href="https://colab.research.google.com/drive/1c8fmSa1H9noi_1RJhFOksloEFNDrEmU7?usp=sharing">implementing
RNNs</a></p>
]]></content>
      <categories>
        <category>dlsys</category>
      </categories>
      <tags>
        <tag>rnn</tag>
      </tags>
  </entry>
  <entry>
    <title>DLSys: Transformers and Attention</title>
    <url>/2024/08/27/DLSys-Transformers-and-Autoregressive-Models/</url>
    <content><![CDATA[<h2 id="theory">Theory</h2>
<h3 id="the-two-approaches-to-time-series-modeling">The two approaches
to time series modeling</h3>
<p>Let‚Äôs recall our basic time series prediction task from the previous
posts. More fundamentally, a time series prediction task is the task of
predicting: <span class="math display">\[
y_{1:T}=f_\theta(x_{1:T})
\]</span>
<img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240828130234284.png" alt="image-20240828130234284" style="zoom:50%;" /></p>
<p>where <span class="math inline">\(y_t\)</span> can depend only on
<span class="math inline">\(x_{1:t}\)</span>. There are mainly two
approach to do so: <strong>latent state approach</strong> and
<strong>direct prediction</strong>.</p>
<h4 id="the-rnn-latent-state-approach">The RNN ‚Äúlatent state‚Äù
approach</h4>
<p>We have already seen the RNN approach to time series:
<strong>maintain ‚Äúlatent state‚Äù <span class="math inline">\(h_t\)</span>
that summarizes all information up until that point</strong>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240828130718884-20240828130724791.png" alt="image-20240828130718884" style="zoom: 33%;" /></p>
<ul>
<li><strong>Pros</strong>: Potentially ‚Äúinfinite‚Äù history, compact
representation. It means the time series length is not limited and has
low memory to store the previous context.</li>
<li><strong>Cons</strong>: Long ‚Äúcompute path‚Äù between history and
current time ‚üπ vanishing / exploding gradients, hard to learn. A single
state <span class="math inline">\(h_t\)</span> is hard to represent long
context, easily lost the previous context.</li>
</ul>
<h4 id="the-direct-prediction-approach">The ‚Äúdirect prediction‚Äù
approach</h4>
<p>To avoid vanishing/exploding gradients(lose context/context is hard
to store), we can also directly predict output <span
class="math inline">\(y_t\)</span>: <span class="math display">\[
y_t = f_\theta(x_{1:t})
\]</span> <span class="math inline">\(f_\theta\)</span> must be a
function that can make predictions of differently-sized inputs.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240828131152642.png" alt="image-20240828131152642" style="zoom:33%;" /></p>
<ul>
<li><strong>Pros</strong>: Often can map from past to current state with
shorter compute path. With a proper function <span
class="math inline">\(f_\theta\)</span> , we can avoid
vanishing/exploding gradients.</li>
<li><strong>Cons</strong>: No compact state representation, finite
history in practice.</li>
</ul>
<p>One of the most straightforward ways to specify the function <span
class="math inline">\(f_\theta\)</span>: (fully) convolutional networks,
a.k.a. temporal convolutional networks (TCNs). The main constraint is
that <strong>the convolutions be causal</strong>: <span
class="math inline">\(z^{i+1}_t\)</span> can only depend on <span
class="math inline">\(z^{i}_{t-k:t}\)</span>.</p>
<p>Many successful applications: e.g. WaveNet for speech generation (<a
href="https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/">van
den Oord et al., 2016</a>)</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240828132255771.png" alt="image-20240828132255771" style="zoom:67%;" /></p>
<p>Despite their simplicity, CNNs have a notable disadvantage for time
series prediction: the receptive field of each convolution is usually
relatively small ‚üπ need deep networks to actually incorporate past
information. For example, WaveNet can achive a receptive field of 16
using 4 layer. For very long sequence, it will need very deep network.
There are several solutions:</p>
<ul>
<li><strong>Increase kernel size</strong>: also increases the parameters
of the network</li>
<li><a
href="https://medium.com/@abhishekjainindore24/pooling-and-their-types-in-cnn-4a4b8a7a4611"><strong>Pooling
layers</strong></a>: not as well suited to dense prediction, where we
want to predict all of <span class="math inline">\(y_{1:T}\)</span>.
We'll lose some predictions because pooling will decrease the size of
input.(alose decrease the size of output)</li>
<li><strong>Dilated convolutions</strong>: ‚ÄúSkips over‚Äù some past state
/ inputs. But we'll lose some context which may be important.</li>
</ul>
<p>As we can see, CNN is not well suited for time series prediction.
We'll introduce a new arch of network: transformer which will overcome
the cons of CNN.</p>
<h3 id="self-attention-and-transformers">Self-attention and
Transformers</h3>
<h4 id="self-attention">Self Attention</h4>
<p>Let's first talk about the important part of transformer:
<strong>Self Attention</strong>! ‚ÄúAttention‚Äù in deep networks generally
refers to <strong>any mechanism where individual states are weighted and
then combined</strong>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829085656637.png" alt="image-20240829085656637" style="zoom:50%;" /></p>
<p>Attention is used originally in RNNs when one wanted to combine
latent states over all times in a more general manner than ‚Äújust‚Äù
looking at the last state. Let's define general attention in math: <span
class="math display">\[
z_t = \theta^Th_t^k
\]</span></p>
<p><span class="math display">\[
w = softmax(z)
\]</span></p>
<p><span class="math display">\[
\bar{h} = \sum_{t=1}^T(w_th_t^k)
\]</span></p>
<blockquote>
<p>The <strong>softmax function</strong> converts a vector of <em>K</em>
real numbers into a <a
href="https://en.wikipedia.org/wiki/Probability_distribution">probability
distribution</a> of <em>K</em> possible outcomes.</p>
</blockquote>
<p><strong>Self-attention</strong> refers to a particular form of
attention mechanism. Given three inputs <span
class="math inline">\(K,Q,V \in R^{T \times d}\)</span>, (‚Äúqueries‚Äù,
‚Äúkeys‚Äù, ‚Äúvalues‚Äù, in one of the least-meaningful semantic designations
we have in deep learning).</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829090838397.png" alt="image-20240829090838397" /></p>
<p>we define the self attention operation as: <span
class="math display">\[
SelfAttetion(Q,K,V)=softmax(\frac{QK^T}{d^{1/2}})V
\]</span> Where the input is <span class="math inline">\(X \in R^{T
\times n}, W_k \in W^{n \times d}, W_Q \in W^{n \times d}, W_V \in W^{n
\times d}\)</span>, we can simple calculate <span
class="math inline">\(Q, K, V\)</span> as follows: <span
class="math display">\[
Q = XW_Q
\]</span></p>
<p><span class="math display">\[
K = XW_K
\]</span></p>
<p><span class="math display">\[
V = XW_V
\]</span></p>
<p>Compare to the attention used in rnn, we use input to replace the
hidden state <span class="math inline">\(h\)</span>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829092012276.png" alt="image-20240829092012276" style="zoom:50%;" /></p>
<p>Properties of self-attention:</p>
<ul>
<li>Invariant (really, equivariant) to permutations of the <span
class="math inline">\(Q, K, V\)</span>matrices</li>
<li>Allows influence between <span class="math inline">\(q_t,k_t,
v_t\)</span>over all times without increase parameter size.(compare to
CNN, in order to increase reception field, we need to increase kernel
size --&gt; increase parameter size)</li>
<li>Compute cost is <span class="math inline">\(O(T^2 + 2Td)\)</span>
(cannot be easily reduced due to nonlinearity applied to full <span
class="math inline">\(T \times T\)</span> matrix)</li>
<li>softmax const <span class="math inline">\(T^2\)</span></li>
<li>two matrix multiplication cost <span
class="math inline">\(Td\)</span></li>
</ul>
<h4 id="transformer">Transformer</h4>
<p>A simple transoformer block consist of self-attention mechanism and
other network blocks.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829092949134.png" alt="image-20240829092949134" style="zoom:50%;" /></p>
<p>In more detail, the Transformer block has the following form:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829093117580.png" alt="image-20240829093117580" style="zoom: 33%;" /></p>
<p>The Transformer architecture uses a series of attention mechanisms
(and feedfoward layers) to process a time series:</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829093229868.png" alt="image-20240829093229868" style="zoom: 50%;" /></p>
<p>Which can be form use math equation below: <span
class="math display">\[
Z^{(i+1)}=TransformerBlock(Z^{(i)})
\]</span> All time steps (in practice, within a given time slice) are
<strong>processed in parallel</strong>, avoids the need for sequential
processing as in RNNs.</p>
<p>We can apply the Transformer block to the ‚Äúdirect‚Äù prediction method
for time series, instead of using a convolutional block.</p>
<ul>
<li>Pros:
<ul>
<li>Full receptive field within a single layer (i.e., can immediately
use past data)</li>
<li>Mixing over time doesn‚Äôt increase parameter count (unlike
convolutions)</li>
</ul></li>
<li>Cons:
<ul>
<li>All outputs depend on all inputs (no good e.g., for autoregressive
tasks) -- the latent cortex is more important.</li>
<li>No ordering of data (remember that transformers are equivariant to
permutations of the sequence)--the position masters.</li>
</ul></li>
</ul>
<p>To solve the cons of transformer, we introduce two techniques:
<strong>masked self-attention</strong>, and <strong>Positional
encodings</strong>.</p>
<h4 id="masked-self-attention">Masked self-attention</h4>
<p>To solve the problem of ‚Äúacausal‚Äù dependencies, we can mask the
softmax operator to assign zero weight to any ‚Äúfuture‚Äù time steps.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829094005050.png" alt="image-20240829094005050" style="zoom: 50%;" /></p>
<p>Note that even though technically this means we can ‚Äúavoid‚Äù creating
those entries in the attention matrix to being with, in practice it‚Äôs
often faster to just form them then mask them out.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829094241280.png" alt="image-20240829094241280" style="zoom:50%;" /></p>
<h4 id="positional-encodings">Positional encodings</h4>
<p>To solve the problem of ‚Äúorder invariance‚Äù, we can add a positional
encoding to the input, which <strong>associates each input with its
position in the sequence</strong>.</p>
<p><img src="https://cdn.jsdelivr.net/gh/xjh42/oss@main/uPic/image-20240829094534849.png" alt="image-20240829094534849" style="zoom:50%;" /></p>
<p>and where <span class="math inline">\(w_i, i = 1,...,n\)</span> is
typically chosen according to a logarithmic schedule. Really, add
positional encoding to d-dimensional projection of <span
class="math inline">\(T\)</span></p>
<h3 id="transformers-beyond-time-series">Transformers beyond time
series</h3>
<p>Recent work has observed that transformer blocks are extremely
powerful beyond just time series</p>
<ul>
<li>Vision Transformers: Apply transformer to image (represented by a
collection of patch embeddings), works better than CNNs for large data
sets</li>
<li>Graph Transformers: Capture graph structure in the attention
matrix</li>
</ul>
<p>In all cases, some challenges are:</p>
<ul>
<li>How to represent data such that <span
class="math inline">\(O(T^2)\)</span> operations are feasible</li>
<li>How to form positional embeddings</li>
<li>How to form the mask matrix</li>
</ul>
<h2 id="implementation">Implementation</h2>
<p>The runnable colab implementation is <a
href="https://colab.research.google.com/drive/1sSkoJhexTDEgdBahAIm2SeV0Eqc0frj2?usp=sharing">here</a></p>
]]></content>
      <categories>
        <category>dlsys</category>
      </categories>
      <tags>
        <tag>transformer</tag>
      </tags>
  </entry>
</search>
